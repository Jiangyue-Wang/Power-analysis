########## using "pwr" package
install.packages("pwr")
library("pwr")

######## using t.test as an example
?pwr.t.test
t1 <- pwr.t.test(d = 0.5,
                 sig.level = 0.05,
                 power = 0.8,
                 type = "paired",
                 alternative = "two.sided")
t1
plot(t1)
###### what do the different parameters mean?
###### note that standard deviation/variance is inherently embodied in d (along with the "difference")
###### excellent reference on effect size: https://www.theanalysisfactor.com/sample-size-most-difficult-step/

###### let's play with the values of the parameters a bit more
#### lowering (or upping) effect size
#### lowering (or upping) alpha (type I error)
#### lowering (or upping) power (1-type II error)
#### changing the type of the test (paired versus two samples)
#### changing the "alternative" of the test (one- or two-tailed/sided)
###### what did you find - how does n change?

###### let's play with which parameter you intend to estimate
#### what about supplying n and estimating power?
#### what about supplying n and estimating effect size?
###### what did you find - does this help you understand power analysis a bit better?

###### one way to come up with a rough idea of d (effect size) is to use the cohen.ES() function
?cohen.ES
cohen.ES(test ="t", 
         size = "medium")
#### this is what would be typically considered as a medium-sized effect for Cohen's d
#### try playing with different sizes (changing "medium" to "small" or "large") and see the difference

######## using anova as an example
cohen.ES(test = "anov",
         size = "medium")

###### suppose we want to compare 3 types of hand wash in their effectiveness in reducing germs
?pwr.anova.test
t2 <- pwr.anova.test(k = 3,
                     f = 0.25,
                     sig.level = 0.05,
                     power = 0.9)
t2
plot(t2)
###### what do the different parameters mean?

###### let's play with the values of the parameters a bit more
#### lowering (or upping) the number of groups
#### lowering (or upping) effect size
#### lowering (or upping) alpha (type I error)
#### lowering (or upping) power (1-type II error)
###### what did you find - how does n change?

###### let's play with which parameter you intend to estimate
#### what about supplying n and estimating power?
#### what about supplying n and estimating effect size?
###### what did you find - does this help you understand power analysis a bit better?

######## using linear regression as an example
###### suppose here is the model we want to consider: y ~ x1 + x2
?pwr.f2.test
#### u: number of coefficients in the model (not counting the intercept)
#### v: = n-u-1 -> n = v+u+1
#### f2: R2/(1-R2) (R2: proportion of variance explained)
#### assuming R2 = 0.5 -> f2 = 1

t3 <- pwr.f2.test(u = 2,
                  f2 = 1,
                  sig.level = 0.05,
                  power = 0.9)
t3
plot(t3) # error message (multiple regression power plotting is not supported...)
###### n=14+2+1=17
###### what do the different parameters mean?

###### let's play with the values of the parameters a bit more
#### lowering (or upping) the number of parameters in the regression
#### lowering (or upping) effect size
#### lowering (or upping) alpha (type I error)
#### lowering (or upping) power (1-type II error)
###### what did you find - how does n change?

###### let's play with which parameter you intend to estimate
#### what about supplying n and estimating power?
#### what about supplying n and estimating effect size?
###### what did you find - does this help you understand power analysis a bit better?

######## very good reference on using the "pwr" package:
######## https://cran.r-project.org/web/packages/pwr/vignettes/pwr-vignette.html 




########## using "paramtest" package
install.packages("paramtest")
install.packages("dplyr")
install.pacakges("ggplot2")
library("paramtest")
library("dplyr")
library("ggplot2")

######## the mechanism of conducting power analysis using simulation
###### Simulations repeatedly generate random data based on one’s predefined model,
###### then analyze each data set and count the proportion of results that are significant. 
###### That proportion is the estimated power for the model

######## the way that "paramtest" package works
###### "paramtest" makes this process simple by providing a general-purpose, flexible function to
###### perform simulations. You simply create a function that generates the data and analyses you want, 
###### and then use one of the functions to run your user-defined function repeatedly and collate the results.

######## repeating power analysis for t.test as an example
###### using "pwr" package:
t1 <- pwr.t.test(d = 0.5,
                 sig.level = 0.05,
                 #power = 0.8,
                 n = 50,
                 type = "two.sample",
                 alternative = "two.sided")
t1

###### using "paramtest" package:
#### create user-defined function to generate and analyze data
t_func <- function(simNum, N, d) {
  x1 <- rnorm(N, 0, 1)
  x2 <- rnorm(N, d, 1)
  t <- t.test(x1, x2, var.equal=TRUE) # run t-test on generated data
  stat <- t$statistic
  p <- t$p.value
  return(c(t=stat, p=p, sig=(p < .05)))
  # return a named vector with the results we want to keep
}
power_ttest <- run_test(t_func, 
                        n.iter=5000, 
                        output="data.frame", 
                        N=50, 
                        d=0.5) # simulate data
results(power_ttest) %>% summarise(power=mean(sig))
#### what happened above:
## We first create a function that simulates normally distributed data for two groups, and performs a t-test. 
## The t statistic and p-value are then returned as a named vector, along with a boolean value determining whether 
## the test is significant or not. Finally, we use the run_test() function to perform 5000 simulations, and then
## summarize the ‘sig’ value, which (by default) calculates the mean, giving us the proportion of simulations that
## were significant. This number agrees very closely with the analytic solution above.

#### varying parameters above
#### give 'params' a list of parameters we want to vary
## testing at N=25, N=50, and N=100
power_ttest_vary <- grid_search(t_func, 
                                params=list(N=c(25, 50, 100)),
                                n.iter=5000, 
                                output='data.frame', 
                                d=0.5)
results(power_ttest_vary) %>% group_by(N.test) %>% summarise(power=mean(sig))

## testing at N=25, N=50, and N=100 and varying d at the same time
# varying N and Cohen's d
power_ttest_vary2 <- grid_search(t_func, 
                                 params=list(N=c(25, 50, 100), 
                                             d=c(0.2, 0.5)),
                                 n.iter=5000, 
                                 output="data.frame")
power <- results(power_ttest_vary2) %>% group_by(N.test, d.test) %>% summarise(power=mean(sig))
print(power)
ggplot(power, 
       aes(x=N.test, 
           y=power, 
           group=factor(d.test), 
           colour=factor(d.test))) +
  geom_point() +
  geom_line() +
  ylim(c(0, 1)) +
  labs(x="Sample Size", y="Power", colour="Cohen's d") +
  theme_minimal()

######## power analysis for multilevel models (i.e. models with correlated data structures)
######## this is something the "pwr" package cannot handle

###### an example on repeated measures model, with data taken at 4 different time points

mlm_test <- function(simNum, N, b1, b0=0, xm=0, xsd=1, varInt=1, varSlope=1, varResid=1) {
  timePoints <- 4
  subject <- rep(1:N, each=timePoints)
  sub_int <- rep(rnorm(N, 0, sqrt(varInt)), each=timePoints) # random intercept
  sub_slope <- rep(rnorm(N, 0, sqrt(varSlope)), each=timePoints) # random slope
  time <- rep(0:(timePoints-1), N)
  y <- (b0 + sub_int) + 
       (b1 + sub_slope)*time +
       rnorm(N*timePoints, 0, sqrt(varResid))
  # y-intercept as a function of b0 plus random intercept;
  # slope as a function of b1 plus random slope
  
  data <- data.frame(subject, sub_int, sub_slope, time, y)
  
  # for more complex models that might not converge, tryCatch() is probably a good idea
  return <- tryCatch({
    model <- nlme::lme(y ~ time, random=~time|subject, data=data)
    # when using parallel processing, we must refer to functions from packages directly, 
    # e.g., package::function()
    est <- summary(model)$tTable['time', 'Value']
    se <- summary(model)$tTable['time', 'Std.Error']
    p <- summary(model)$tTable['time', 'p-value']
    return(c(est=est, se=se, p=p, sig=(p < .05)))
  },
  error=function(e) {
    #message(e) # print error message
    return(c(est=NA, se=NA, p=NA, sig=NA))
  })
  return(return)
}

# 500 iterations are used so that the document compiles faster; it is recommended however,
# that more iterations are used for a stable estimate

power_mlm <- grid_search(mlm_test, 
                         params=list(N=c(200, 300)), 
                         n.iter=500,
                         output="data.frame", 
                         b1=0.15,
                         varInt=0.05, 
                         varSlope=0.15, 
                         varResid=0.4)
power <- results(power_mlm) %>% group_by(N.test) %>% 
  summarise(power=mean(sig, na.rm=TRUE),
            na=sum(is.na(sig))) # we use this to count up how many cases did not properly converge
print(power)
ggplot(power, 
       aes(x=N.test, 
           y=power)) +
  geom_point() +
  geom_line() +
  ylim(c(0, 1)) +
  labs(x="Sample Size", y="Power") +
  theme_minimal()

######## very good references on using the "paramtest" package:
######## https://cran.r-project.org/web/packages/paramtest/vignettes/Simulating-Power.html
######## also: https://www.psycharchives.org/bitstream/20.500.12034/1736/2/Power_analyses__v2_.html


######## for the "simR" package, the following are good references:
###### https://benwhalley.github.io/just-enough-r/power-analysis.html#for-most-inferential-statistics 
###### https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.12504

